{"name":"Konschti.GitHub.io","tagline":"Submission for Practical Machine Learning","body":"Final Project Practical Machine Learning\r\n\r\nFirst, the training data is loaded. All columns that are containing NAs are deleted. The same applies for columns which seem to be useless for the prediction such as time stamps and user names. After this the entire training data is randomly divided into a training and a testing subset whereby the training subset contains 70% of the overall training samples.\r\n\r\nAs it is explained during week 3 of our course boosting and random forests are the two modelling approaches that are said to exhibit the best performance in prediction contests. Therefore, I have chosen to focus my analysis on these two. \r\n\r\nI have decided to train two different models on the training subset and then to evaluate these two models on the testing subset. The one which will achieve a higher accuracy on the testing subset is to be used also for the “real” prediction in the testing set.\r\n\r\nAll variables are included in the training process of the two models.\r\n\r\nA five-fold cross validation is performed three times to find the best model parameters for both boosting and random forests. These two resulting models are evaluated on the remaining 30% of the original training data which has not been used for model training. The measured criterion is the accuracy of the prediction. I would expect this pseudo-out-of-sample error to be a reliable predictor of the real out-of-sample error. For the “boosting” model the accuracy is 0.9629 while for the “random forest”-model the accuracy is 0.9941.\r\n\r\nFinally I have used the model with the higher accuracy (the random forest) for the prediction on the test data. \r\n\r\n ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}